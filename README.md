# üöÄ AstraRAG ‚Äì Agentic RAG Chatbot

A fully modular **Agentic RAG Chatbot** built using **CrewAI** , **FastAPI** , **LlamaIndex** , **ChromaDB** , and a **Streamlit Frontend** .

This system combines:

- üîπ **Agentic reasoning** (CrewAI)
- üîπ **RAG pipeline** with Chroma vector database
- üîπ **FastAPI backend** for orchestration
- üîπ **Streamlit UI** for chatbot interaction
- üîπ **Clean architecture** (agents ‚Üí tasks ‚Üí services ‚Üí API ‚Üí UI)

---

# üì∏ Screenshots

### üîπ Streamlit UI

![1766936010007](image/README/1766936010007.png)

![1766936020505](image/README/1766936020505.png)

### üîπ Folder Structure

![1766935981284](image/README/1766935981284.png)

---

# ‚ú® Features

- ü§ñ **Agentic Q&A chatbot** powered by CrewAI
- üìÑ **Document ingestion pipeline** with LlamaIndex
- üß† **ChromaDB vector storage** with persistent embeddings
- ‚ö° **FastAPI backend** to connect agents with frontend
- üé® **Streamlit frontend UI**
- üîç **Search + RAG retrieval + agent reasoning**
- üß© Modular file structure for scalability
- üõ† Detailed logging for debugging

---

# üèó Tech Stack

### **Core**

- Python 3.10+
- CrewAI
- LlamaIndex
- FastAPI
- Streamlit

### **Vector DB**

- **ChromaDB**

### **Embeddings**

- HuggingFace Embeddings

### **Frontend**

- Streamlit

---

# üìÅ Project Folder Structure

```
AstraRAG-agentic-rag-chatbot/
‚îÇ
‚îú‚îÄ‚îÄ docs_dir/                     # Raw input documents
‚îú‚îÄ‚îÄ doc_vector_store/             # ChromaDB persistent embeddings
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents_src/               # CrewAI agents & tasks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tasks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crew.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ rag_doc_ingestion/        # RAG ingestion pipeline (LlamaIndex)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingest_docs.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ backend_src/              # FastAPI backend
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ frontend_src/             # Streamlit UI
‚îÇ       ‚îú‚îÄ‚îÄ main.py
‚îÇ       ‚îú‚îÄ‚îÄ config/
‚îÇ
‚îî‚îÄ‚îÄ venv/
```

---

# üß† RAG Ingestion Pipeline

This pipeline converts raw documents into searchable vector embeddings.

### üîπ Steps:

1. **Load documents** from `docs_dir/`
2. **Parse into text chunks** using LlamaIndex NodeParser
3. **Generate embeddings** using HuggingFace Embeddings
4. **Store embeddings** in ChromaDB (persistent directory)
5. Create a **VectorStoreIndex**

### ‚ú® Ingestion Code (Simplified)

```python
loader = SimpleDirectoryReader(input_dir=docs_dir)
documents = loader.load_data()

parser = SimpleNodeParser.from_defaults(chunk_size=1024, chunk_overlap=50)
nodes = parser.get_nodes_from_documents(documents)

vector_store = ChromaVectorStore(chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

index = VectorStoreIndex(
    nodes,
    storage_context=storage_context,
    embed_model=embed_model
)
```

---

# üîÑ Backend Architecture (FastAPI ‚Üí Services ‚Üí CrewAI ‚Üí Response)

This is the heart of your system.
Here is the clear workflow from frontend to backend to agent and back.

---

## üî• **Flow Overview**

### **1Ô∏è‚É£ User sends a query (Streamlit ‚Üí FastAPI)**

Streamlit sends:

```
POST /chat/answer
```

Payload:

```json
{
  "chat_history": [{ "role": "user", "content": "Explain Quantum physics" }]
}
```

---

## **2Ô∏è‚É£ FastAPI Router Receives Request**

```python
@router.post("/chat/answer")
def chat_answer(request: ChatHistoryRequest):
    result = get_answer(chat_history)
    return result
```

---

## **3Ô∏è‚É£ Service Layer Processes Query**

`services/chat.py`:

```python
last_user_message = chat_history[-1]
user_query = last_user_message["content"]

result = qa_crew.kickoff({
    "user_query": user_query,
    "chat_history": history_without_last
})
```

This prepares:

- last user message
- chat history (context)
- input to CrewAI

---

## **4Ô∏è‚É£ Agentic Processing (CrewAI Agent + Task)**

CrewAI receives `user_query` and `chat_history`
Then:

- üß† Uses LlamaIndex retriever
- üîç Fetches top vector embeddings from Chroma
- üìù Writes reasoning trace
- üéØ Generates final answer

Crew returns a structured dict:

```json
{
  "answer": "...",
  "sources": ["doc1.txt"],
  "tool_used": "retrieval",
  "rationale": "Used RAG + agent reasoning"
}
```

---

## **5Ô∏è‚É£ FastAPI Returns JSON to Frontend**

Backend sends structured response.

---

## **6Ô∏è‚É£ Streamlit Renders UI**

UI shows:

- Assistant reply
- Sources
- Tool used
- Rationale (inside expander)

---

# üìä Request Flow Diagram (Mermaid)

```mermaid
sequenceDiagram
    participant UI as Streamlit UI
    participant API as FastAPI Backend
    participant SRV as Service Layer
    participant AG as CrewAI Agent
    participant DB as ChromaDB

    UI->>API: POST /chat/answer (chat_history)
    API->>SRV: call get_answer(chat_history)
    SRV->>AG: kickoff(user_query, history)
    AG->>DB: retrieve relevant vectors
    DB-->>AG: return embeddings
    AG-->>SRV: final answer + sources
    SRV-->>API: return result dict
    API-->>UI: JSON response
```

---

# üß™ Running the Project

## 1Ô∏è‚É£ Install dependencies

```bash
pip install -r requirements.txt
```

## 2Ô∏è‚É£ Run RAG ingestion

```bash
python src/rag_doc_ingestion/ingest_docs.py
```

## 3Ô∏è‚É£ Start FastAPI backend

```bash
uvicorn src.backend_src.main:app --reload
```

## 4Ô∏è‚É£ Start Streamlit frontend

```bash
streamlit run src/frontend_src/main.py
```

---

# üóÇ Environment Variables

Create `.env` file:

```
DOCUMENTS_DIR=docs_dir
VECTOR_STORE_DIR=doc_vector_store
COLLECTION_NAME=astra_collection

API_HOST=127.0.0.1
API_PORT=8000
CHAT_ENDPOINT_URL=http://127.0.0.1:8000/chat/answer
```

---

# ‚≠ê Future Enhancements

- Add multi-agent workflow
- Add tool calling (browser, calculator, etc.)
- Add authentication for backend
- Deploy to cloud (AWS/GCP/Render/EC2)

---

# üôå About

Built by **Ashutosh Raj Gupta**
Designed for **Agentic RAG + LLM engineering practice**
